{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "(This post has been edited after Philippe Lonjoux's insightful comment. The amendments I made are highlighted in my answer to his comment.)\n\n\nHello there!\n\nThis notebook exploits the analysis I did [here][1] to boost the performance of a classifier.\n\nIn particular, I will show how additional features computed from 'manager_id' can substantially improve performances by building upon the neat [\"Random Forest Starter\"][2] by Li Li\n\n  [1]: https://www.kaggle.com/den3b81/two-sigma-connect-rental-listing-inquiries/do-managers-matter-some-insights-on-manager-id\n  [2]: https://www.kaggle.com/aikinogard/two-sigma-connect-rental-listing-inquiries/random-forest-starter-with-numerical-features",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ...let's import the modules\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ... and load the training data\ndf = pd.read_json(open(\"../input/train.json\", \"r\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Naive feature engineering (from Li Li's Random Forest Starter)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\n\nfeatures_to_use = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n                   \"num_photos\", \"num_features\", \"num_description_words\",\n                   \"created_year\", \"created_month\", \"created_day\"]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Basic encoding of 'manager_id', from SRK [\"XGBoost starter\"][1] \n\n\n  [1]: https://www.kaggle.com/sudalairajkumar/two-sigma-connect-rental-listing-inquiries/xgb-starter-in-python",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn import preprocessing\n\nlbl = preprocessing.LabelEncoder()\nlbl.fit(list(df['manager_id'].values))\ndf['manager_id'] = lbl.transform(list(df['manager_id'].values))\n\n# let's add this feature\nfeatures_to_use.append('manager_id')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Let's now add 3 columns contaning the fractions of 'low','medium' and 'high' interest level obtained by each manager in the training dataset. \n\n### We also add the simple 'manager_skill' feature I introduced in my [previous notebook][1].\n\n### We use mean values for those managers who don't have enough entries for being ranked (minimum = 20 here).\n\n\n----------\n\n\n### As pointed out by Philippe Lonjoux's, since these new features involve the target variable, we split the dataset first so to avoid \"cheating\" during the validation phase. The features are computed for each manager on the training part and the values obtained are then copied for the instances in the validation dataset. \n\n\n  [1]: https://www.kaggle.com/den3b81/two-sigma-connect-rental-listing-inquiries/do-managers-matter-some-insights-on-manager-id.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's split the data\nX = df[features_to_use]\ny = df[\"interest_level\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# compute fractions and count for each manager\ntemp = pd.concat([X_train.manager_id,pd.get_dummies(y_train)], axis = 1).groupby('manager_id').mean()\ntemp.columns = ['high_frac','low_frac', 'medium_frac']\ntemp['count'] = X_train.groupby('manager_id').count().iloc[:,1]\n\n# remember the manager_ids look different because we encoded them in the previous step \nprint(temp.tail(10))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# compute skill\ntemp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac']\n\n# get ixes for unranked managers...\nunranked_managers_ixes = temp['count']<20\n# ... and ranked ones\nranked_managers_ixes = ~unranked_managers_ixes\n\n# compute mean values from ranked managers and assign them to unranked ones\nmean_values = temp.loc[ranked_managers_ixes, ['high_frac','low_frac', 'medium_frac','manager_skill']].mean()\nprint(mean_values)\ntemp.loc[unranked_managers_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\nprint(temp.tail(10))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# inner join to assign manager features to the managers in the training dataframe\nX_train = X_train.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\nX_train.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# add the features computed on the training dataset to the validation dataset\nX_val = X_val.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\nnew_manager_ixes = X_val['high_frac'].isnull()\nX_val.loc[new_manager_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\nX_val.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# add manager fractions and skills to the features to use\nfeatures_to_use.extend(['high_frac','low_frac', 'medium_frac','manager_skill'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Let's train and validate a few random forest classifiers to see whether we can improve performances with thee additional features",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Basic model with no manager-related features, this is the model in Li Li's Random Forest Starter",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# features to use for this classifier == only basic numerical\nthese_features = [f for f in features_to_use if f not in ['manager_id','high_frac','low_frac', 'medium_frac','manager_skill']]\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train[these_features], y_train)\ny_val_pred = clf.predict_proba(X_val[these_features])\nlog_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's visualize features importance, \n# price is the most important feature, followed by number of descriptive words, latitude and longitude\npd.Series(index = these_features, data = clf.feature_importances_).sort_values().plot(kind = 'bar')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Let's add manager_id and see if we can get some improvement already",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# add manager_id\nthese_features = [f for f in features_to_use if f not in ['high_frac','low_frac', 'medium_frac','manager_skill']]\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train[these_features], y_train)\ny_val_pred = clf.predict_proba(X_val[these_features])\nlog_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's visualize features importance\npd.Series(index = these_features, data = clf.feature_importances_).sort_values().plot(kind = 'bar')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### We get a small improvement, but we can do better. Let's remove 'manager_id' and use manager interest fractions and skill instead",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# no manager_id, use fractions and skill instad\nthese_features = [f for f in features_to_use if f not in ['manager_id']]\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train[these_features], y_train)\ny_val_pred = clf.predict_proba(X_val[these_features])\nlog_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's visualize features importance\npd.Series(index = these_features, data = clf.feature_importances_).sort_values().plot(kind = 'bar')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### That's an improvement, but maybe we can do better if we just use the fractions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# no manager_id, no skill, use fractions\nthese_features = [f for f in features_to_use if f not in ['manager_id','manager_skill']]\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train[these_features], y_train)\ny_val_pred = clf.predict_proba(X_val[these_features])\nlog_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's visualize features importance\npd.Series(index = these_features, data = clf.feature_importances_).sort_values().plot(kind = 'bar')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### What if we use the manager skill and not the fractions?!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# no manager_id, no fraction, use skill instead\nthese_features = [f for f in features_to_use if f not in ['manager_id','high_frac','low_frac', 'medium_frac']]\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train[these_features], y_train)\ny_val_pred = clf.predict_proba(X_val[these_features])\nlog_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's visualize features importance\npd.Series(index = these_features, data = clf.feature_importances_).sort_values().plot(kind = 'bar')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Oh cool, manager_skill still does boost our performances further and it is almost as important as the price feature.\n\n### Therefore, I suggest you to use this feature in your classifier, no matter its nature. I am confident you gonna get a nice boost if you are not including similar features already in your model.\n\n\n### Cheers",
      "metadata": {}
    }
  ]
}